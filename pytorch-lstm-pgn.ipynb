{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Chess engine\n",
    "A totally nonsensical chess engine based off learning on PGN data.\n",
    "- No checks for legal moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sequence_length = 4\n",
    "batch_size = 200\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fully_connected = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fully_connected(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGN parsing\n",
    "txt = Path(\"./datasets/pgn_small.txt\").read_text()\n",
    "games = txt.split(\"\\n\")\n",
    "clean_moves = []\n",
    "words = []\n",
    "sequence_list = []\n",
    "c = 0\n",
    "for game in games:\n",
    "    if c >= 100:\n",
    "        break\n",
    "        \n",
    "    if game == \"\":\n",
    "        continue\n",
    "    moves = re.split(\"\\d+\\.\", game)\n",
    "    \n",
    "    # Don't include short games\n",
    "    if len(moves) < 10:\n",
    "        continue\n",
    "        \n",
    "    c = c + 1\n",
    "    clean_moves = [m.strip() for m in moves if m != \"\"]\n",
    "    seq = []\n",
    "    for move in clean_moves:\n",
    "        m = move.split(\" \")\n",
    "        words.append(m[0])\n",
    "        words.append(m[1])\n",
    "        seq.append(m[0])\n",
    "        seq.append(m[1])\n",
    "    sequence_list.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, words, sequence_list):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.words = words\n",
    "        self.sequence_list = sequence_list\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "        # Preprocess sequences as in getitem\n",
    "        # for each sequence generate windows\n",
    "        self.data = []\n",
    "        for seq in sequence_list:\n",
    "            for idx in range(len(seq) - self.sequence_length):\n",
    "                input = seq[idx : idx + self.sequence_length]\n",
    "                output = seq[idx + 1 : idx + self.sequence_length + 1]                \n",
    "                input2 = [self.word_to_index[w] for w in input]\n",
    "                output2 = [self.word_to_index[w] for w in output]\n",
    "                # print(f\"{input2}, {output2}\")\n",
    "                self.data.append([input2, output2])\n",
    "        \n",
    "        print(f\"Total sequences {len(sequence_list)}\")\n",
    "        print(f\"Unique moves {len(self.uniq_words)}\")\n",
    "        print(f\"Data length {len(self.data)}\")\n",
    "\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "    \n",
    "        return (\n",
    "            torch.tensor(x[0]),\n",
    "            torch.tensor(x[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, epochs):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    print(f\"Total batchs {len(dataloader)}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch + 1) % 100 == 0:\n",
    "                print({ 'epoch': epoch, 'batch': batch + 1, 'loss': loss.item() })\n",
    "        print({ 'epoch': epoch, 'batch': batch + 1, 'loss': loss.item() })\n",
    "    print(\"All done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=1):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences 100\n",
      "Unique moves 1071\n",
      "Data length 6104\n",
      "Total batchs 31\n",
      "{'epoch': 0, 'batch': 31, 'loss': 6.710749626159668}\n",
      "{'epoch': 1, 'batch': 31, 'loss': 6.1540446281433105}\n",
      "{'epoch': 2, 'batch': 31, 'loss': 5.93922233581543}\n",
      "{'epoch': 3, 'batch': 31, 'loss': 5.905770301818848}\n",
      "{'epoch': 4, 'batch': 31, 'loss': 6.232414722442627}\n",
      "{'epoch': 5, 'batch': 31, 'loss': 5.9441070556640625}\n",
      "{'epoch': 6, 'batch': 31, 'loss': 5.647998332977295}\n",
      "{'epoch': 7, 'batch': 31, 'loss': 5.270970821380615}\n",
      "{'epoch': 8, 'batch': 31, 'loss': 4.948829650878906}\n",
      "{'epoch': 9, 'batch': 31, 'loss': 4.686614036560059}\n",
      "{'epoch': 10, 'batch': 31, 'loss': 4.235520362854004}\n",
      "{'epoch': 11, 'batch': 31, 'loss': 3.987637519836426}\n",
      "{'epoch': 12, 'batch': 31, 'loss': 3.9004037380218506}\n",
      "{'epoch': 13, 'batch': 31, 'loss': 3.7037806510925293}\n",
      "{'epoch': 14, 'batch': 31, 'loss': 3.233628511428833}\n",
      "{'epoch': 15, 'batch': 31, 'loss': 2.8027713298797607}\n",
      "{'epoch': 16, 'batch': 31, 'loss': 2.4834933280944824}\n",
      "{'epoch': 17, 'batch': 31, 'loss': 2.1439778804779053}\n",
      "{'epoch': 18, 'batch': 31, 'loss': 1.8977630138397217}\n",
      "{'epoch': 19, 'batch': 31, 'loss': 1.6597949266433716}\n",
      "{'epoch': 20, 'batch': 31, 'loss': 1.402211308479309}\n",
      "{'epoch': 21, 'batch': 31, 'loss': 1.2806785106658936}\n",
      "{'epoch': 22, 'batch': 31, 'loss': 1.0937000513076782}\n",
      "{'epoch': 23, 'batch': 31, 'loss': 0.822415292263031}\n",
      "{'epoch': 24, 'batch': 31, 'loss': 0.7913249135017395}\n",
      "{'epoch': 25, 'batch': 31, 'loss': 0.7232102155685425}\n",
      "{'epoch': 26, 'batch': 31, 'loss': 0.6631607413291931}\n",
      "{'epoch': 27, 'batch': 31, 'loss': 0.5518680214881897}\n",
      "{'epoch': 28, 'batch': 31, 'loss': 0.5263550877571106}\n",
      "{'epoch': 29, 'batch': 31, 'loss': 0.4757991135120392}\n",
      "All done!!!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(words, sequence_list)\n",
    "model = Model(dataset)\n",
    "train(dataset, model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e4', 'e5', 'Nf3', 'Nc6', 'Nc3']\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model, text='e4 e5 Nf3 Nc6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
